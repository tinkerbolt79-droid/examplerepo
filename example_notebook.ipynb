{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXnjs7OKV6z9TVxhsEKROg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tinkerbolt79-droid/examplerepo/blob/main/example_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJbIt9auxU4v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "1cca290b-552a-485d-c41b-2adf53424280"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "################################################################          89.8%"
          ]
        }
      ],
      "source": [
        "# Download and install ollama to the system\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok ollama"
      ],
      "metadata": {
        "id": "WtWF6Q1NFCte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess;\n",
        "\n",
        "def start_ollama_server() -> None:\n",
        "    \"\"\"Starts the Ollama server.\"\"\"\n",
        "    subprocess.Popen(['ollama', 'serve'])\n",
        "    print(\"Ollama server started.\")\n",
        "\n",
        "start_ollama_server()"
      ],
      "metadata": {
        "id": "RlDOTD1kFHxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install pyngrok once (Colab will keep it for this session)\n",
        "import google.colab.userdata as userdata\n",
        "from pyngrok import ngrok   # <-- import here\n",
        "\n",
        "def setup_ngrok_tunnel(port: str) -> ngrok.NgrokTunnel:\n",
        "    \"\"\"Sets up an ngrok tunnel.\n",
        "\n",
        "    Args:\n",
        "        port: The port to tunnel.\n",
        "\n",
        "    Returns:\n",
        "        The ngrok tunnel object.\n",
        "\n",
        "    Raises:\n",
        "        RuntimeError: If the ngrok authtoken is not set.\n",
        "    \"\"\"\n",
        "    ngrok_auth_token = userdata.get('NGROK_AUTHTOKEN')\n",
        "    if not ngrok_auth_token:\n",
        "        raise RuntimeError(\"NGROK_AUTHTOKEN is not set.\")\n",
        "\n",
        "    ngrok.set_auth_token(ngrok_auth_token)\n",
        "    tunnel = ngrok.connect(port, host_header=f'localhost:{port}')\n",
        "    print(f\"ngrok tunnel created: {tunnel.public_url}\")\n",
        "    return tunnel"
      ],
      "metadata": {
        "id": "OdM9-xBAFcA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NGROK_PORT = '11434'\n",
        "ngrok_tunnel = setup_ngrok_tunnel(NGROK_PORT)"
      ],
      "metadata": {
        "id": "2ccG8t5vHGkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_ollama_with_client(prompt: str, model_id: str, img_path: list = None) -> None:\n",
        "    \"\"\"Queries the Ollama server using the `ollama-python` client library.\n",
        "\n",
        "    Args:\n",
        "        prompt: The prompt to send to the model.\n",
        "        model_id: The ID of the model to use.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        messages=[\n",
        "            {\n",
        "                'role': 'user',\n",
        "                'content': prompt,\n",
        "            },\n",
        "        ]\n",
        "\n",
        "        if img_path:\n",
        "          messages[0]['images'] = [img_path]\n",
        "\n",
        "        response = client.chat(\n",
        "            model=model_id,\n",
        "            messages=messages\n",
        "        )\n",
        "        print(\"Response from ollama client:\")\n",
        "        print(response['message']['content'])\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying Ollama with client: {e}\")\n",
        "\n",
        "MODEL_ID='gemma3:4b'\n",
        "PROMPT='Why is the sky blue?'\n",
        "query_ollama_with_client(PROMPT, MODEL_ID)"
      ],
      "metadata": {
        "id": "hrkq1EJQM3F4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}